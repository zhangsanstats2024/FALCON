{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import *\n",
    "import L0_card\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000\n",
    "batch_size = 16\n",
    "seed = 0\n",
    "model_name = \"mnist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Started logging run 20220705_13-35-01_426047_64 of experiment mlpnet, saving checkpoints every 1 epoch\n",
      "this is config before OrderedDict([('pruners', OrderedDict([('pruner_1', OrderedDict([('class', 'WoodburryFisherPruner'), ('epochs', [0, 2, 2]), ('weight_only', True), ('initial_sparsity', 0.0), ('target_sparsity', 0.9), ('modules', ['fc2']), ('keep_pruned', True)]))])), ('trainers', OrderedDict([('default_trainer', OrderedDict([('optimizer', OrderedDict([('class', 'SGD'), ('lr', 0.001), ('momentum', 0.5)])), ('lr_scheduler', OrderedDict([('class', 'ExponentialLR'), ('gamma', 0.9), ('epochs', [120, 1, 140])]))]))]))])\n",
      "Overwrite arguments for one-shot!\n",
      "this is config after OrderedDict([('pruners', OrderedDict([('pruner_1', OrderedDict([('class', 'BlockwiseWoodburryFisherPruner'), ('epochs', [0, 1, 1]), ('weight_only', True), ('initial_sparsity', 0.9), ('target_sparsity', 0.9), ('modules', ['fc1', 'fc2', 'fc3']), ('keep_pruned', True)]))])), ('trainers', OrderedDict([('default_trainer', OrderedDict([('optimizer', OrderedDict([('class', 'SGD'), ('lr', 0.001), ('momentum', 0.5)])), ('lr_scheduler', OrderedDict([('class', 'ExponentialLR'), ('gamma', 0.9), ('epochs', [120, 1, 140])]))]))]))])\n",
      "total epochs is 1\n",
      "True, not_oldfashioned\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Also printing results@top5 as well!\n",
      "Do log softmax:  True\n",
      "=======Layers:=============\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengx/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:122: UserWarning: \n",
      "    Found GPU0 Tesla K20m which is of cuda capability 3.5.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    The minimum cuda capability supported by this library is 3.7.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn % (d, name, major, minor, min_arch // 10, min_arch % 10))\n",
      "Module .fc1 was successfully wrapped\n",
      "Module .fc2 was successfully wrapped\n",
      "Module .fc3 was successfully wrapped\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_model_config {'arch': 'mlpnet', 'dataset': 'mnist', 'use_butterfly': False}\n",
      "Do log softmax:  True\n",
      "updated_state_dict odict_keys(['fc1._weight_mask', 'fc1._layer.weight', 'fc2._weight_mask', 'fc2._layer.weight', 'fc3._weight_mask', 'fc3._layer.weight'])\n",
      "args is  Namespace(aa=False, always_eval_test=False, arch='mlpnet', aux_gpu_id=-1, batch_size=64, batched_test=False, cache_subset_ids=False, centered=False, check_grads=False, check_train_loss=False, checkpoint_freq=1, ckpt_epoch=-1, compare_globalmagni_mask=False, compare_models=False, config_path='../WoodFisher/configs/mlpnet_mnist_config_one_shot_woodburry_fisher.yaml', cpu=True, deterministic=True, device=device(type='cpu'), disable_bias=True, disable_log_soft=False, disable_train_random_transforms=False, disable_train_shuffle=False, disable_wdecay_after_prune=False, dset='mnist', dset_path='./datasets', dump_fisher_inv_mat=False, dump_grads_mat=False, enable_dropout=False, epochs=1, eps=1e-10, eval_fast=False, exp_dir='../exp_root/mlpnet', exp_name='mlpnet', experiment_root_path='../exp_root', export_onnx=False, fisher_cpu=False, fisher_damp=1e-05, fisher_damp_correction=False, fisher_effective_damp=False, fisher_mini_bsz=1, fisher_optimized=False, fisher_parts=5, fisher_seed=-1, fisher_split_grads=False, fisher_subsample_size=100, fisher_trace=False, fittable_params=-1, flops=False, flops_epsilon=1, flops_normalize=None, flops_per_param=False, flops_power=0, flops_target=-1, from_checkpoint_path='../WoodFisher/checkpoints/mnist_25_epoch_93.97.ckpt', full_subsample=True, gpus=-1, grad_subsample_size=None, hess_label_smoothing=None, ignore_prefix=False, init_sparsity=0.9, inspect_inv=False, kernel_sizes=3, kfac_pi=False, label_smoothing=0, layer_trace_stat=False, load_distiller_weights_from=None, load_fisher='', local_quadratic=False, logging_level=10, mask_onnx=False, max_mini_bsz=None, no_dataparallel=False, normalize_hgp=False, normalize_update=False, normalize_update_mult=1, not_oldfashioned=True, num_classes=10, num_hidden_nodes1=40, num_hidden_nodes2=20, num_path_steps=-1, num_samples=100, offload_grads=False, offload_inv=False, old_fashioned=False, one_shot=True, onnx_nick=None, p=3, pretrained=False, previous_mask=False, prune_all=False, prune_at_launch=False, prune_bias=False, prune_class='woodfisherblock', prune_direction=False, prune_end=1, prune_freq=1, prune_lr=None, prune_modules='fc1_fc2_fc3', prune_momentum=None, prune_optimizer=None, prune_start=0, prune_wdecay=0, recompute_bn_stats=False, recompute_degree=None, recompute_num=None, recompute_schedule=None, repeated_one_shot=False, report_top5=True, reset_training_policy=False, result_file='./tests/results_1657042501.csv', run_dir='../exp_root/mlpnet/20220705_13-35-01_426047_64', run_id='20220705_13-35-01_426047_64', save_before_prune_ckpt=False, save_dense_also=False, save_fisher=False, scale_prune_update=1, se_ratio=None, seed=1, set_prune_momentum=False, spearman_globalmagni=False, subtract_min=False, sweep_id=64, target_sparsity=0.9, test_batch_size=None, topk=False, training_stats_freq=30, true_fisher=False, untrained_last=False, update_config=True, use_butterfly=False, use_model_config=True, use_se=False, woodburry_joint_sparsify=True, woodtaylor_abs=False, workers=1, zero_after_prune=False)\n",
      "Copy param module.fc1._weight_mask with shape torch.Size([40, 784]) in checkpoint to fc1._weight_mask\n",
      "Copy param module.fc1._layer.weight with shape torch.Size([40, 784]) in checkpoint to fc1._layer.weight\n",
      "Copy param module.fc2._weight_mask with shape torch.Size([20, 40]) in checkpoint to fc2._weight_mask\n",
      "Copy param module.fc2._layer.weight with shape torch.Size([20, 40]) in checkpoint to fc2._layer.weight\n",
      "Copy param module.fc3._weight_mask with shape torch.Size([10, 20]) in checkpoint to fc3._weight_mask\n",
      "Copy param module.fc3._layer.weight with shape torch.Size([10, 20]) in checkpoint to fc3._layer.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Constructed BlockwiseWoodburryFisherPruner with config:\n",
      "    -epochs:[0, 1, 1]\n",
      "    -weight_only:True\n",
      "    -initial_sparsity:0.9\n",
      "    -target_sparsity:0.9\n",
      "    -modules:['fc1', 'fc2', 'fc3']\n",
      "    -keep_pruned:True\n",
      "\n",
      "IN BLOCK WOODBURRY\n",
      "initial sparsity in args is 0.9\n",
      "initial sparsity in config is 0.9\n",
      "../WoodFisher/models/mlpnet.py:45: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---computing gradients--  0\n",
      "---computing gradients--  100\n",
      "---computing gradients--  200\n",
      "---computing gradients--  300\n",
      "---computing gradients--  400\n",
      "---computing gradients--  500\n",
      "---computing gradients--  600\n",
      "---computing gradients--  700\n",
      "---computing gradients--  800\n",
      "---computing gradients--  900\n"
     ]
    }
   ],
   "source": [
    "X, w_bar, model, train_dataloader, test_dataloader, modules_to_prune = load_Xw(model_name, sample_size, batch_size, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntar is  3235\n",
      "Shape of X is  (1000, 32360)\n"
     ]
    }
   ],
   "source": [
    "y = X@w_bar\n",
    "p = w_bar.shape[0]\n",
    "beta = w_bar\n",
    "r = y - X@beta\n",
    "sparsity = 0.9\n",
    "ntot = w_bar.shape[0]\n",
    "ntar = int(ntot * (1-sparsity))\n",
    "print(\"ntar is \",ntar)\n",
    "print(\"Shape of X is \", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important Parameters:\n",
    "\n",
    "+ k: number of non-zeros\n",
    "+ alpha,lambda1,lambda2,M, beta_tilde1,beta_tilde2: parameters of the objective $F(\\beta)$ (see Cardinality-heuristics.tex in overleaf)\n",
    "+ L: estimation of Lipschitz condition of $\\nabla f(\\beta)$ (just set L=None and the alg will compute it by itself)\n",
    "+ iht_max_itr: maximal number of inner iterations\n",
    "+ ftol: stopping criteria of inner loop\n",
    "+ act_max_itr: maximal number of outer iterations\n",
    "+ buget,kimp: the initial cardinality of the active set is set to be $\\max(k, \\min(p,kimp*k,buget))$\n",
    "+ act_itr: initial IHT iterations used to generate the active set\n",
    "+ cd_itr: number of CD iteration performed in each inner loop\n",
    "+ ctol: starting criteria of CD iteration \n",
    "+ sea1_max_itr: number of line search performed in each inner loop\n",
    "+ sea2_max_itr: number of line search performed when updating the active set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.copy(w_bar)\n",
    "k = ntar\n",
    "beta, f, objs, r, cur_iter, sol_time = L0_card.Vanilla_IHTCD(y,X,beta,k,alpha=np.zeros(p),lambda1=0.001,lambda2=0.01,M=np.inf, beta_tilde1=np.copy(w_bar),\n",
    "            beta_tilde2=np.copy(w_bar), L=None, iht_max_itr=600,ftol=1e-6,cd_itr=1,ctol=1e-4,search_max_itr=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(186.94732856750488, 0.7746456911085802, 600)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sol_time, f , cur_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), 93.97, tensor(0.9000), 92.94)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_imp(model, beta, test_dataloader, modules_to_prune=modules_to_prune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengx/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:368: FutureWarning: If 'random_state' is not supplied, the current default is to use 0 as a fixed seed. This will change to  None in version 1.2 leading to non-deterministic results that better reflect nature of the randomized_svd solver. If you want to silence this warning, set 'random_state' to an integer seed or to None explicitly depending if you want your code to be deterministic or not.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of iter: 1  num of inner iter: 600 \n",
      " Finding new active set\n",
      "0.7749946368206647 0.7749946640805259 0 1\n",
      "0.7749946126842256 0.7749946640805259 0 2\n",
      "0.7749945737816218 0.7749946640805259 0 3\n",
      "0.7749945334574897 0.7749946640805259 0 4\n",
      "0.7749946027335374 0.7749946640805259 0 5\n",
      "0.7749953409828836 0.7749946640805259 0 6\n"
     ]
    }
   ],
   "source": [
    "beta = np.copy(w_bar)\n",
    "k = ntar\n",
    "beta, f, sols, r, cur_iter, sol_time = L0_card.Active_IHTCD(y,X,beta,k,alpha=np.zeros(p),lambda1=0.001,lambda2=0.01,M=np.inf, beta_tilde1=np.copy(w_bar), \n",
    "                     beta_tilde2=np.copy(w_bar), L=None, iht_max_itr=600, ftol = 1e-6, act_max_itr=10, buget=None, kimp=2, act_itr=100,\n",
    "                     cd_itr = 5, ctol = 1e-4, sea1_max_itr=5, sea2_max_itr=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29.032485723495483, 0.7749946640805259, 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sol_time, f , cur_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), 93.97, tensor(0.9000), 93.06)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_imp(model, beta, test_dataloader, modules_to_prune=modules_to_prune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to deal the case in which X has zero column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first 100 columen of X is 1\n",
    "np.random.seed(seed=2)\n",
    "X = np.hstack([np.zeros((100,100)),np.random.randn(100,1000)])\n",
    "w_bar = np.random.randn(1100)\n",
    "y = X@w_bar\n",
    "p = 1100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengx/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:368: FutureWarning: If 'random_state' is not supplied, the current default is to use 0 as a fixed seed. This will change to  None in version 1.2 leading to non-deterministic results that better reflect nature of the randomized_svd solver. If you want to silence this warning, set 'random_state' to an integer seed or to None explicitly depending if you want your code to be deterministic or not.\n",
      "  warnings.warn(\n",
      "/home/mengx/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:368: FutureWarning: If 'random_state' is not supplied, the current default is to use 0 as a fixed seed. This will change to  None in version 1.2 leading to non-deterministic results that better reflect nature of the randomized_svd solver. If you want to silence this warning, set 'random_state' to an integer seed or to None explicitly depending if you want your code to be deterministic or not.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ffa2ff4c2ca7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_bar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m beta, f, sols, r, cur_iter, sol_time = L0_card.Active_IHTCD(y,X,beta,100,alpha=np.zeros(p),lambda1=0.001,lambda2=0,M=np.inf, beta_tilde1=np.copy(w_bar), \n\u001b[0m\u001b[1;32m      5\u001b[0m                      \u001b[0mbeta_tilde2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_bar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miht_max_itr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mftol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_max_itr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkimp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_itr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                      cd_itr = 5, ctol = 1e-4, sea1_max_itr=5, sea2_max_itr=10)\n",
      "\u001b[0;32m~/netprune/L0_card.py\u001b[0m in \u001b[0;36mActive_IHTCD\u001b[0;34m(y, X, beta, k, alpha, lambda1, lambda2, beta_tilde1, beta_tilde2, L, M, iht_max_itr, ftol, act_max_itr, buget, kimp, act_itr, cd_itr, ctol, sea1_max_itr, sea2_max_itr)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mL_act\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.05\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskl_svd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_act\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlambda2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m         beta_act, f, objs, r_act, iht_cur_itr, sol_time = Vanilla_IHTCD(y,X_act,beta_act,k,alpha[active_set],lambda1,lambda2,beta_tilde1[active_set],\n\u001b[0m\u001b[1;32m    477\u001b[0m                                                                      beta_tilde2[active_set], L_act,M,iht_max_itr,ftol,cd_itr,ctol,sea1_max_itr)\n\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/netprune/L0_card.py\u001b[0m in \u001b[0;36mVanilla_IHTCD\u001b[0;34m(y, X, beta, k, alpha, lambda1, lambda2, beta_tilde1, beta_tilde2, L, M, iht_max_itr, ftol, cd_itr, ctol, search_max_itr)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_old\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mctol\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0miht_cur_itr\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCD_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mS_diag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta_tilde1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta_tilde2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcd_itr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta_tilde1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta_tilde2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# direct apply IHTCD (or active IHTCD) yields divide by 0 error\n",
    "\n",
    "beta = np.copy(w_bar)\n",
    "beta, f, sols, r, cur_iter, sol_time = L0_card.Active_IHTCD(y,X,beta,100,alpha=np.zeros(p),lambda1=0.001,lambda2=0,M=np.inf, beta_tilde1=np.copy(w_bar), \n",
    "                     beta_tilde2=np.copy(w_bar), L=None, iht_max_itr=600, ftol = 1e-6, act_max_itr=10, buget=None, kimp=2, act_itr=100,\n",
    "                     cd_itr = 5, ctol = 1e-4, sea1_max_itr=5, sea2_max_itr=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengx/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:368: FutureWarning: If 'random_state' is not supplied, the current default is to use 0 as a fixed seed. This will change to  None in version 1.2 leading to non-deterministic results that better reflect nature of the randomized_svd solver. If you want to silence this warning, set 'random_state' to an integer seed or to None explicitly depending if you want your code to be deterministic or not.\n",
      "  warnings.warn(\n",
      "/home/mengx/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:368: FutureWarning: If 'random_state' is not supplied, the current default is to use 0 as a fixed seed. This will change to  None in version 1.2 leading to non-deterministic results that better reflect nature of the randomized_svd solver. If you want to silence this warning, set 'random_state' to an integer seed or to None explicitly depending if you want your code to be deterministic or not.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of iter: 1  num of inner iter: 600 \n",
      " Finding new active set\n",
      "2.0310441597160707 2.0335625093232723 0 1\n",
      "2.0287579987827176 2.0335625093232723 0 2\n",
      "2.0248822429374096 2.0335625093232723 0 3\n",
      "2.0199169953325575 2.0335625093232723 0 4\n",
      "2.0211315564658143 2.0335625093232723 0 5\n",
      "2.068140904104333 2.0335625093232723 0 6\n"
     ]
    }
   ],
   "source": [
    "# Apply IHTCD (or active IHTCD) with a pre-processing procedure solves the problem\n",
    "\n",
    "beta = np.copy(w_bar)\n",
    "beta, f, sols, r, cur_iter, sol_time = L0_card.Active_IHTCD_PP(y,X,beta,100,alpha=np.zeros(p),lambda1=0.001,lambda2=0,M=np.inf, beta_tilde1=np.copy(w_bar), \n",
    "                     beta_tilde2=np.copy(w_bar), L=None, iht_max_itr=600, ftol = 1e-6, act_max_itr=10, buget=None, kimp=2, act_itr=100,\n",
    "                     cd_itr = 5, ctol = 1e-4, sea1_max_itr=5, sea2_max_itr=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengx/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:368: FutureWarning: If 'random_state' is not supplied, the current default is to use 0 as a fixed seed. This will change to  None in version 1.2 leading to non-deterministic results that better reflect nature of the randomized_svd solver. If you want to silence this warning, set 'random_state' to an integer seed or to None explicitly depending if you want your code to be deterministic or not.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "beta, f, objs, r, iht_cur_itr, sol_time = L0_card.Vanilla_IHTCD_PP(y,X,beta,100,alpha=np.zeros(p),lambda1=0.001,lambda2=0,M=np.inf, beta_tilde1=np.copy(w_bar), \n",
    "                     beta_tilde2=np.copy(w_bar), L=None, iht_max_itr=600, ftol = 1e-6, cd_itr=0,ctol=1e-4,search_max_itr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
